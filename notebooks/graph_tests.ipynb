{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29ed8f6b-84c6-49c7-a0c2-a4de00078840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from hvectorspaces.io import PostgresClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eaa5bf64-5622-43b9-8753-85c057658595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1930,2030,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fed9449d-12c1-4430-be10-558885c75f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    \"title\", \n",
    "    \"publication_year\", \n",
    "    \"domain\", \n",
    "    \"field\", \n",
    "    \"topic\",\n",
    "    \"abstract\"\n",
    "]\n",
    "\n",
    "data_per_decade = list()\n",
    "decades = list(range(1920,2030,10))\n",
    "\n",
    "with PostgresClient() as client:\n",
    "    for d in decades:\n",
    "        citation_map = client.fetch_per_decade_data(d, additional_fields=fields)\n",
    "        id_to_cited_ids = {}\n",
    "        id_to_attrs = {} \n",
    "        for oa_id, refs, title, publication_year, domain, field, topic, abstract in citation_map:\n",
    "            id_to_cited_ids[oa_id] = refs\n",
    "            id_to_attrs[oa_id] = {\n",
    "                \"title\": title,\n",
    "                \"publication_year\": publication_year,\n",
    "                \"domain\": domain,\n",
    "                \"field\": field,\n",
    "                \"topic\": topic,\n",
    "                \"abstract\" : abstract\n",
    "            }\n",
    "        data_per_decade.append(id_to_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "169395e6-dc29-4279-ad88-d5c578d3d427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "32\n",
      "63\n",
      "410\n",
      "1534\n",
      "3462\n",
      "6601\n",
      "20160\n",
      "82622\n",
      "161255\n",
      "60036\n"
     ]
    }
   ],
   "source": [
    "for d in data_per_decade:\n",
    "    print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "93ec9842-68ef-49e5-a93b-7ef739b7cab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1019189097 {'title': 'canonical correlation forests', 'publication_year': 2022, 'domain': 'Physical Sciences', 'field': 'Computer Science', 'topic': 'Neural Networks and Applications', 'abstract': 'We introduce canonical correlation forests (CCFs), a new decision tree ensemble method for classification and regression. Individual canonical correlation trees are binary decision trees with hyperplane splits based on local canonical correlation coefficients calculated during training. Unlike axis-aligned alternatives, the decision surfaces of CCFs are not restricted to the coordinate system of the inputs features and therefore more naturally represent data with correlated inputs. CCFs naturally accommodate multiple outputs, provide a similar computational complexity to random forests, and inherit their impressive robustness to the choice of input parameters. As part of the CCF training algorithm, we also introduce projection bootstrapping, a novel alternative to bagging for oblique decision tree ensembles which maintains use of the full dataset in selecting split points, often leading to improvements in predictive accuracy. Our experiments show that, even without parameter tuning, CCFs out-perform axis-aligned random forests and other state-of-the-art tree ensemble methods on both classification and regression problems, delivering both improved predictive accuracy and faster training times. We further show that they outperform all of the 179 classifiers considered in a recent extensive survey.'}\n",
      "W105055686 {'title': 'the party is over here: structure and content in the 2010 election', 'publication_year': 2021, 'domain': 'Social Sciences', 'field': 'Social Sciences', 'topic': 'Social Media and Politics', 'abstract': 'In this work, we study the use of Twitter by House, Senate and gubernatorial candidates during the midterm (2010) elections in the U.S. Our data includes almost 700 candidates and over 690k documents that they produced and cited in the 3.5 years leading to the elections. We utilize graph and text mining techniques to analyze differences between Democrats, Republicans and Tea Party candidates, and suggest a novel use of language modeling for estimating content cohesiveness. Our findings show significant differences in the usage patterns of social media, and suggest conservative candidates used this medium more effectively, conveying a coherent message and maintaining a dense graph of connections. Despite the lack of party leadership, we find Tea Party members display both structural and language-based cohesiveness. Finally, we investigate the relation between network structure, content and election results by creating a proof-of-concept model that predicts candidate victory with an accuracy of 88.0%.'}\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "for k, v in islice(data_per_decade[-1].items(), 2):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66b50a8e-d8b2-41b6-97dd-b267bc00fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_title_and_abstract(entry:dict[str,str]):\n",
    "    if isinstance(entry['abstract'], str):\n",
    "        if isinstance(entry['title'], str):\n",
    "            return entry['title'] + \"\\n\" + entry['abstract']\n",
    "        else:\n",
    "            return entry['abstract']\n",
    "    elif isinstance(entry['title'], str): \n",
    "        return entry['title']\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d2ee279f-a9db-462e-b435-f82c96724d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "640f9591-ce6a-4254-8bb3-e414dcb20f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_openalex_text(text: str) -> str:\n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Remove zero-width and BOM chars\n",
    "    text = re.sub(r\"[\\u200b\\u200c\\u200d\\ufeff]\", \"\", text)\n",
    "\n",
    "    # Remove LaTeX math blocks\n",
    "    text = re.sub(r\"\\$[^$]+\\$\", \" \", text)\n",
    "\n",
    "    # Remove LaTeX commands (\\alpha, \\mathbb{R}, etc.)\n",
    "    text = re.sub(r\"\\\\[a-zA-Z]+(\\{[^}]*\\})?\", \" \", text)\n",
    "\n",
    "    # Replace math operators & symbols with space\n",
    "    text = re.sub(r\"[∑∂≈⊗⊕≤≥≠∞√±×÷]\", \" \", text)\n",
    "\n",
    "    # Collapse repeated punctuation\n",
    "    text = re.sub(r\"([=+\\-*_]){2,}\", \" \", text)\n",
    "\n",
    "    # Keep only letters, numbers, basic punctuation\n",
    "    text = re.sub(r\"[^0-9A-Za-zÀ-ÿ.,;:()\\-\\s]\", \" \", text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1059f28d-7d4e-4722-9838-820f9bcf1ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920\n",
      "10\n",
      "\n",
      "1930\n",
      "32\n",
      "\n",
      "1940\n",
      "63\n",
      "\n",
      "1950\n",
      "410\n",
      "\n",
      "1960\n",
      "1534\n",
      "\n",
      "1970\n",
      "3462\n",
      "\n",
      "1980\n",
      "6601\n",
      "\n",
      "1990\n",
      "20156\n",
      "\n",
      "2000\n",
      "82605\n",
      "\n",
      "2010\n",
      "161251\n",
      "\n",
      "2020\n",
      "60036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_per_decade = list()\n",
    "\n",
    "for idx, d in enumerate(data_per_decade):\n",
    "    txts = [merge_title_and_abstract(v) for v in d.values()]\n",
    "    txts = [clean_openalex_text(entry.lower()) for entry in txts if isinstance(entry, str)]\n",
    "    txts_split = [entry.split(\" \") for entry in txts if isinstance(entry, str)]\n",
    "    corpus_per_decade.append(txts_split)\n",
    "    print(decades[idx])\n",
    "    print(str(len(txts_split)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ab0392bf-fd79-4445-a6bc-7c73fe446973",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [txt for txts in corpus_per_decade for txt in txts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0357b985-a9f3-48c9-a76a-b124fdde91b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['edge',\n",
       " 'label',\n",
       " 'inference',\n",
       " 'in',\n",
       " 'generalized',\n",
       " 'stochastic',\n",
       " 'block',\n",
       " 'models:',\n",
       " 'from',\n",
       " 'spectral']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ef7cde5d-482e-4d9c-9750-c6f2ac32176f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 23s, sys: 665 ms, total: 1min 24s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#corpus: List[List[str]]\n",
    "bigram = Phrases(\n",
    "    corpus,\n",
    "    min_count=5,\n",
    "    threshold=10,\n",
    "    delimiter=\"_\",   # <-- str, not bytes\n",
    ")\n",
    "bigram_phraser = Phraser(bigram)\n",
    "\n",
    "corpus_bi = [bigram_phraser[doc] for doc in corpus]\n",
    "\n",
    "trigram = Phrases(\n",
    "    corpus_bi,\n",
    "    min_count=5,\n",
    "    threshold=10,\n",
    "    delimiter=\"_\",\n",
    ")\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "corpus_tri = [trigram_phraser[doc] for doc in corpus_bi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "59c3c798-d67e-46de-b1c5-8142a7d8ee8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['edge',\n",
       " 'label',\n",
       " 'inference',\n",
       " 'in',\n",
       " 'generalized',\n",
       " 'stochastic_block',\n",
       " 'models:',\n",
       " 'from',\n",
       " 'spectral',\n",
       " 'theory']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bi[-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cb0d1b66-fd90-4675-880c-2877d309ba9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['edge',\n",
       " 'label',\n",
       " 'inference',\n",
       " 'in',\n",
       " 'generalized',\n",
       " 'stochastic_block',\n",
       " 'models:',\n",
       " 'from',\n",
       " 'spectral',\n",
       " 'theory']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tri[-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ddbc0855-be0e-45a5-a495-80652ac87ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.utils import simple_preprocess\n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "\n",
    "# ---------- (A) Optional: write your tokenized docs to disk (recommended if big) ----------\n",
    "def write_corpus(tokens_iter, out_path: str):\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for doc in tokens_iter:\n",
    "            if not doc:\n",
    "                continue\n",
    "            f.write(\" \".join(doc) + \"\\n\")\n",
    "\n",
    "# If you already have corpus_tri in memory, uncomment:\n",
    "write_corpus(corpus_tri, \"../data/openalex_phrased.txt\")\n",
    "\n",
    "# ---------- (B) Stream sentences from disk (memory-safe) ----------\n",
    "sentences = LineSentence(\"../data/openalex_phrased.txt\")  # one tokenized doc per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "757a4036-7fa1-4009-823a-007350bafdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self, total_epochs):\n",
    "        self.total_epochs = total_epochs\n",
    "        self.epoch = 0\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        elapsed = time.time() - self.start_time\n",
    "        avg_epoch = elapsed / self.epoch\n",
    "        remaining = avg_epoch * (self.total_epochs - self.epoch)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {self.epoch}/{self.total_epochs} | \"\n",
    "            f\"elapsed: {elapsed/60:.1f} min | \"\n",
    "            f\"ETA: {remaining/60:.1f} min\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601322b5-b2a3-4654-b2c3-5b6289ee4a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- (C) Train word2vec ----------\n",
    "workers = max(1, mp.cpu_count() - 1)\n",
    "epochs = 10\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=300,      # 100–300 common; 300 is a solid baseline\n",
    "    window=5,             # try 5–10 for scholarly text\n",
    "    min_count=10,         # raise if huge corpus; lower if small\n",
    "    sg=1,                 # 1=skip-gram (often better); 0=CBOW (faster)\n",
    "    negative=10,\n",
    "    sample=1e-4,\n",
    "    epochs=epochs,\n",
    "    workers=workers,\n",
    "    callbacks=[EpochLogger(epochs)],\n",
    ")\n",
    "\n",
    "# ---------- (D) Save ----------\n",
    "out_dir = Path(\"../models/w2v_openalex_baseline\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "w2v.save(out_dir / \"word2vec.model\")                        # full model\n",
    "w2v.wv.save(out_dir / \"word2vec.kv\")                        # keyed vectors (recommended)\n",
    "w2v.wv.save_word2vec_format(str(out_dir / \"word2vec.vec\"), binary=False) # text vecs (portable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "13655508-4ae1-41bb-aa17-53a74fdc7cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_space → [('vector_spaces', 0.6354857087135315), ('vector-space', 0.6096470355987549), ('semantics:', 0.5951888561248779), ('distributional_word', 0.5766538381576538), ('information_retrieval:', 0.5727638006210327), ('article_free_access_share', 0.5510422587394714), ('arabic_text', 0.5459734201431274), ('contextualized_word', 0.5454588532447815), ('locally_convex_topological', 0.544205904006958), ('random_indexing', 0.5427467823028564), ('multi-word', 0.5399446487426758), ('word_vector', 0.5397936105728149), ('compositional_distributional', 0.5382021069526672), ('document_indexing', 0.5368563532829285), ('cross-language_information_retrieval', 0.535944938659668), ('embeddings:', 0.5334151983261108), ('handwritten_text', 0.5328021049499512), ('formal_concept', 0.5313637256622314), ('relation_paths', 0.528424084186554), ('an_ontology-based', 0.5275920629501343)]\n",
      "\n",
      "\n",
      "hilbert_space → [('hilbert_space,', 0.6895403861999512), ('hilbert_space.', 0.6610992550849915), ('hilbert_spaces.', 0.6379140019416809), ('bounded_linear_operators', 0.6345569491386414), ('operators_acting', 0.6266502737998962), ('bounded_linear_operator', 0.622100830078125), ('projection_operator', 0.6080055832862854), ('diagonalized', 0.6072152853012085), ('laplace_operator', 0.6015275716781616), ('separable_hilbert', 0.601373016834259), ('chern-simons_gauge_theory', 0.6009882092475891), ('semi-groups', 0.6008126139640808), ('finite_dimensional', 0.6005741357803345), ('self-adjoint_operator', 0.6002179980278015), ('bounded_operator', 0.5996339321136475), ('invariant_subspaces', 0.5986780524253845), ('real_line.', 0.5985540747642517), ('quantum_general_relativity', 0.5963568687438965), ('operator_algebras', 0.5959169864654541), ('adjoints', 0.5934798717498779)]\n",
      "\n",
      "\n",
      "banach_space → [('bounded_linear_operators', 0.7596209049224854), ('banach_space,', 0.7464220523834229), ('bounded_linear_operator', 0.7237773537635803), ('linear_operators', 0.7182406187057495), ('semi-groups', 0.7153353691101074), ('banach', 0.7044858932495117), ('banach_space.', 0.7041624188423157), ('strongly_continuous', 0.7035735249519348), ('semigroup.', 0.7008764147758484), ('infinitesimal_generator', 0.6984719634056091), ('banach_spaces', 0.69454026222229), ('R_n', 0.6886539459228516), ('sobolev_space', 0.6875121593475342), ('lebesgue', 0.6869039535522461), ('lebesgue_spaces', 0.6868869662284851), ('strongly_continuous_semigroup', 0.6856802105903625), ('separable_hilbert', 0.6855681538581848), ('function_spaces', 0.6815129518508911), ('bounded_operator', 0.6799458861351013), ('banach_algebra', 0.678281843662262)]\n",
      "\n",
      "\n",
      "embedding_space → [('embedding_space,', 0.6798788905143738), ('latent_subspace', 0.6656043529510498), ('shared_latent', 0.6595852971076965), ('embedding_space.', 0.6573557257652283), ('visual-semantic', 0.6519162654876709), ('latent_spaces', 0.646833598613739), ('latent_representation.', 0.6446409821510315), ('low-dimensional_embedding', 0.6431475877761841), ('cross-modal_correlation', 0.6396252512931824), ('semantic_embedding', 0.6392708420753479), ('semantic_space', 0.6332669258117676), ('low-rank_constraint,', 0.6331532001495361), ('common_subspace', 0.6330068707466125), ('semantic_space.', 0.6273567080497742), ('cross-media_correlation', 0.6264183521270752), ('distance_(emd)', 0.6262809634208679), ('similarity_metric', 0.6244226098060608), ('transfers_knowledge', 0.6239991188049316), ('semantically_consistent', 0.6238641142845154), ('latent_space,', 0.6221426725387573)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- (E) Quick sanity checks ----------\n",
    "for q in [\"vector_space\", \"hilbert_space\", \"banach_space\", \"embedding_space\"]:\n",
    "    if q in w2v.wv:\n",
    "        print(q, \"→\", w2v.wv.most_similar(q, topn=20))\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(q, \"not in vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4bc0ab63-ff24-4d1d-9a7f-0c555f926e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "for src, targets in id_to_cited_ids.items():\n",
    "    for tgt in targets:\n",
    "        G.add_edge(src, tgt)\n",
    "        \n",
    "nx.set_node_attributes(G, id_to_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1c2ca584-ba5c-4199-9a01-16e66746f1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2478"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "30cc583d-59b4-402e-9607-d8124c90cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinks = [n for n in G.nodes if G.out_degree(n) == 0]\n",
    "sources = [n for n in G.nodes if G.in_degree(n) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "39528cf3-b9ef-4e0a-b6c9-02ec67ab357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "874"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "dc3ddaaa-16d4-4f67-a505-0e95cd8fc5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7fe088d5-c3e9-47d3-88e4-4dc42ba3fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [attrs.get(\"topic\") if attrs.get(\"topic\") is not None else \"None\" for _, attrs in G.nodes(data=True)]\n",
    "domains = [attrs.get(\"domain\") if attrs.get(\"domain\") is not None else \"None\" for _, attrs in G.nodes(data=True)]\n",
    "fields = [attrs.get(\"field\") if attrs.get(\"field\") is not None else \"None\" for _, attrs in G.nodes(data=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2c037f2b-abe8-4911-8f80-a8b30facedbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('None', 625),\n",
       " ('Particle physics theoretical and experimental studies', 75),\n",
       " ('Quantum Chromodynamics and Particle Interactions', 52),\n",
       " ('Cosmology and Gravitation Theories', 49),\n",
       " ('Black Holes and Theoretical Physics', 48),\n",
       " ('Matrix Theory and Algorithms', 43),\n",
       " ('Rings, Modules, and Algebras', 39),\n",
       " ('Electron and X-Ray Spectroscopy Techniques', 38),\n",
       " ('Mathematical Dynamics and Fractals', 32),\n",
       " ('Optimization and Variational Analysis', 28),\n",
       " ('Advanced Statistical Methods and Models', 26),\n",
       " ('Holomorphic and Operator Theory', 25),\n",
       " ('Spectral Theory in Mathematical Physics', 25),\n",
       " ('Quantum Mechanics and Applications', 24),\n",
       " ('Geometric Analysis and Curvature Flows', 24),\n",
       " ('Atomic and Molecular Physics', 22),\n",
       " ('High-pressure geophysics and materials', 22),\n",
       " ('Advanced Algebra and Geometry', 21),\n",
       " ('Advanced Topics in Algebra', 21),\n",
       " ('Economic theories and models', 21),\n",
       " ('Stability and Controllability of Differential Equations', 21),\n",
       " ('advanced mathematical theories', 21),\n",
       " ('Quantum chaos and dynamical systems', 20),\n",
       " ('Advanced Thermodynamics and Statistical Mechanics', 19),\n",
       " ('Target Tracking and Data Fusion in Sensor Networks', 19),\n",
       " ('Theoretical and Computational Physics', 18),\n",
       " ('Advanced Mathematical Modeling in Engineering', 17),\n",
       " ('Game Theory and Voting Systems', 17),\n",
       " ('Advanced Chemical Physics Studies', 17),\n",
       " ('Quantum Electrodynamics and Casimir Effect', 16)]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(topics).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "68f58bb2-c6b4-4813-a14c-10c4918bba2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Physical Sciences', 1623),\n",
       " ('None', 625),\n",
       " ('Social Sciences', 162),\n",
       " ('Life Sciences', 55),\n",
       " ('Health Sciences', 13)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(domains).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f1424b68-03a0-49ed-b393-7c09d3237c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('None', 625),\n",
       " ('Physics and Astronomy', 528),\n",
       " ('Mathematics', 500),\n",
       " ('Computer Science', 214),\n",
       " ('Engineering', 185),\n",
       " ('Materials Science', 95),\n",
       " ('Economics, Econometrics and Finance', 89),\n",
       " ('Earth and Planetary Sciences', 59),\n",
       " ('Decision Sciences', 39),\n",
       " ('Chemistry', 23),\n",
       " ('Agricultural and Biological Sciences', 20),\n",
       " ('Neuroscience', 19),\n",
       " ('Environmental Science', 17),\n",
       " ('Business, Management and Accounting', 15),\n",
       " ('Biochemistry, Genetics and Molecular Biology', 15),\n",
       " ('Medicine', 11),\n",
       " ('Social Sciences', 9),\n",
       " ('Psychology', 7),\n",
       " ('Arts and Humanities', 3),\n",
       " ('Energy', 2)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(fields).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bfa44840-cb1e-4249-a2f9-7c34046b8e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5163"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs = [t for tgts in id_to_cited_ids.values() for t in tgts]\n",
    "len(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6d7c3e-20b4-449e-b743-90f45aced944",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = []\n",
    "for r in refs:\n",
    "    check.append(r in to_nodes)\n",
    "    \n",
    "Counter(check)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvectorspaces",
   "language": "python",
   "name": "hvectorspaces"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
